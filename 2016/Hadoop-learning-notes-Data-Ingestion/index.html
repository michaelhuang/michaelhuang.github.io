<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Hadoop Learning Notes: Data Ingestion · learn to love less</title><meta name="description" content="Hadoop Learning Notes: Data Ingestion - huangzhf"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="http://fonts.useso.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://twitter.com/ZhengfeiHuang" target="_blank" class="nav-list-link">TWITTER</a></li><li class="nav-list-item"><a href="https://github.com/michaelhuang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Hadoop Learning Notes: Data Ingestion</h1><div class="post-info">Apr 15, 2016</div><div class="post-content"><p>数据导入的时候需要考虑哪些因素? <code>No Silver Bullet.</code></p>
<ul>
<li>什么数据类型？数据格式是否和已用的查询处理工具兼容？</li>
<li>预估文件大小如何？是否均匀分布？会不会有大量的小文件？</li>
<li>schema是否要频繁变更？</li>
<li>查询的时候倾向于就那么几列，还是所有列？</li>
<li>数据导入速度要求，及下游使用（比如查询）可容忍程度如何？</li>
<li>集群平台自建？还是别的厂商？<br>…</li>
</ul>
<a id="more"></a>
<h2 id="Data-Ingestion-Considerations"><a href="#Data-Ingestion-Considerations" class="headerlink" title="Data Ingestion Considerations"></a>Data Ingestion Considerations</h2><h6 id="Timeliness-of-data-ingestion-and-accessibility"><a href="#Timeliness-of-data-ingestion-and-accessibility" class="headerlink" title="Timeliness of data ingestion and accessibility"></a>Timeliness of data ingestion and accessibility</h6><p>数据导入频率什么级别，需要多久能供下游使用？</p>
<table>
<thead>
<tr>
<th>classifications</th>
<th>timeliness requirements</th>
<th>tools</th>
<th>storage layer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Macro batch</td>
<td>&gt;    15 minutes</td>
<td>Sqoop, file transfers</td>
<td>HDFS</td>
</tr>
<tr>
<td>Microbatch</td>
<td>&gt;     2 minutes</td>
<td>Sqoop, file transfers</td>
<td>HDFS</td>
</tr>
<tr>
<td>Near-Real-Time Decision Support</td>
<td>&gt;     2 seconds</td>
<td>Flume, Kafka</td>
<td>HBase, Elasticsearch</td>
</tr>
<tr>
<td>Near-Real-Time Event Processing</td>
<td>&gt;   100 milliseconds</td>
<td>Flume, Kafka</td>
<td>HBase, Elasticsearch</td>
</tr>
<tr>
<td>Real Time</td>
<td></td>
<td>custom</td>
<td>custom</td>
</tr>
</tbody>
</table>
<h6 id="Storage-format"><a href="#Storage-format" class="headerlink" title="Storage format"></a>Storage format</h6><p>存储格式主要这几种，纯文本，Hadoop原生的Sequence File，序列化Avro，列存储Parquet和ORC(optimized RCFile)</p>
<table>
<thead>
<tr>
<th>format</th>
<th>size</th>
<th>read</th>
<th>write</th>
<th>feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td>+++++</td>
<td>+</td>
<td>+++++</td>
<td>convenient format, human readable<br>not support block compression<br>Store is bulky and not as efficient to query</td>
</tr>
<tr>
<td>Sequence File</td>
<td>+++</td>
<td>++</td>
<td>+++</td>
<td>Is row oriented format<br>Supports splitting even if the data is compressed<br>Can be used to pack small files in hadoop</td>
</tr>
<tr>
<td>Avro</td>
<td>++</td>
<td>+++</td>
<td>++</td>
<td>Is row oriented binary format<br>Better schema evolution<br>Self-describing and language-independent schema(JSON), the file contain the schema in addition to the data<br>Support block compression and are splittable<br>Compact and fast binary format, widely used as a serialization platform</td>
</tr>
<tr>
<td>Parquet</td>
<td>+</td>
<td>+++++</td>
<td>+</td>
<td>Is column oriented binary format<br>Slow in writing but fast in reading<br>Optimized and efficient in terms of disk I/O when specific columns need to be queried<br>Supports compression<br>Support limited schema evolution, new columns can be added at the end of the structure</td>
</tr>
<tr>
<td>ORC</td>
<td>+</td>
<td>+++++</td>
<td>+</td>
<td>like Parquet, but designed specifically for Hive, not a general-purpose storage format<br>Not support schema evolution</td>
</tr>
</tbody>
</table>
<p>频繁变更的，比如schema、查询结果字段集，Avro更合适；如果查询结果集字段很固定，对写入速度没有要求，Parquet更合适；只想快速导入到HDFS，Text无疑是最快的，但是通常情况下没有人会这样做，比如xml，not splittable，意味着不能并行处理，再就是很耗磁盘网络IO，通常都要压缩，但是又不支持block compression(压缩后not splittable)；ORC是HortonWorks开发的，厂商竞争导致Cloudera Impala用不了，所以Cloudera和twitter一起搞出个Parquet；SequenceFiles类似于csv，但是支持block compression，通常作为MapReduce jobs的过程存储；</p>
<p>格式的选择没那么绝对，选了这个，就不能选那个了，通常都会配合使用，比如processing阶段使用SequenceFiles，query阶段使用Parquet，extract可能选择csv(方便导入数据库)</p>
<h6 id="Codecs"><a href="#Codecs" class="headerlink" title="Codecs"></a>Codecs</h6><p>压缩格式如何选择？</p>
<table>
<thead>
<tr>
<th>codecs</th>
<th>size</th>
<th>compression speed</th>
<th>decompression speed</th>
<th>splittable</th>
</tr>
</thead>
<tbody>
<tr>
<td>Snappy</td>
<td>+++++</td>
<td>++++</td>
<td>++++++</td>
<td>not inherently splittable, need a container format like SequenceFiles or Avro</td>
</tr>
<tr>
<td>LZO</td>
<td>++++</td>
<td>++++</td>
<td>+++++</td>
<td>splittable, requires an additional indexing step</td>
</tr>
<tr>
<td>Gzip</td>
<td>++</td>
<td>++</td>
<td>+++</td>
<td>not inherently splittable, need a container format like SequenceFiles or Avro<br>using smaller blocks can lead to better performance</td>
</tr>
<tr>
<td>bzip2</td>
<td>+</td>
<td>+</td>
<td>++</td>
<td>inherently splittable</td>
</tr>
</tbody>
</table>
<p>从表中可以看出，大多数情况hot data使用Snappy即可，如果是纯文本则使用LZO会更方便一些；cold data建议Gzip；追求更高的压缩比建议bzip2</p>
<h6 id="Incremental-updates"><a href="#Incremental-updates" class="headerlink" title="Incremental updates"></a>Incremental updates</h6><p>新数据以什么形式落地？append or need modifying existing data?<br>只是append，那直接灌到HDFS即可；如果需要merge，对于HDFS来说，需要一个Compact job，对增量数据和现有数据做merge，比如用Hive的left join</p>
<h6 id="Data-access-pattern"><a href="#Data-access-pattern" class="headerlink" title="Data access pattern"></a>Data access pattern</h6><p>batch processing jobs? Or is random access?<br>如果是大量的扫描批处理，那HDFS更合适；如果是随机的访问数据，那NoSQL(比如Hbase)更合适</p>
<h6 id="Source-system-and-data-structure"><a href="#Source-system-and-data-structure" class="headerlink" title="Source system and data structure"></a>Source system and data structure</h6><p>数据源类型：RDBMS or Logs？数据类型：structured、semistructured or unstructured?</p>
<ul>
<li><a href="https://github.com/alexholmes/hdfs-file-slurper" target="_blank" rel="external">hdfs-file-slurper</a><br>轻量级传输工具，多并发，单机多实例，简单容错处理，LZO压缩，定制脚本等</li>
<li><a href="https://sqoop.apache.org/docs/1.99.1/index.html" target="_blank" rel="external">Sqoop</a><br>Hadoop和RDBMS间批量传输利器</li>
<li><a href="http://flume.apache.org/" target="_blank" rel="external">Flume</a><br>日志收集利器</li>
<li><a href="http://kafka.apache.org/" target="_blank" rel="external">Kafka</a>和<a href="https://www.elastic.co/products/logstash" target="_blank" rel="external">logstash</a><br>再加上Elasticsearch, elk日志收集黄金搭档！</li>
</ul>
<h6 id="Network-Bottlenecks"><a href="#Network-Bottlenecks" class="headerlink" title="Network Bottlenecks"></a>Network Bottlenecks</h6><p>做好监控，网卡瓶颈，传输数据压缩</p>
<h6 id="Network-Security"><a href="#Network-Security" class="headerlink" title="Network Security"></a>Network Security</h6><p>数据脱敏，加密解密；如果用Flume，agents之间传输数据支持加密的；用Kafka就需要额外的处理过程</p>
<h6 id="Failure-Handling"><a href="#Failure-Handling" class="headerlink" title="Failure Handling"></a>Failure Handling</h6><p>容错机制：</p>
<ul>
<li>传输10个文件，如果有一个失败了，能否知道是哪个，这样重传的时候只需要传这个失败的就行了</li>
<li>用dfs put传输一个10G大小的文件，传完9G时断了，就得重头来</li>
<li>走队列传，可能会导致一条记录duplicate，接收端需要去重</li>
</ul>
<h6 id="Level-of-Complexity"><a href="#Level-of-Complexity" class="headerlink" title="Level of Complexity"></a>Level of Complexity</h6><p>复杂程度：不要把问题过于复杂化；比如说就是想简单传几个文件上去，就没必要非得搞个Flume或者Kafka集群，直接fs put不就行了；</p>
<p>好的设计不是大而全，而是简而美；不要总想还能加什么，而是可以砍掉什么</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul>
<li><a href="https://www.manning.com/books/hadoop-in-action-second-edition" target="_blank" rel="external">Hadoop in Action</a></li>
<li><a href="http://shop.oreilly.com/product/0636920033196.do" target="_blank" rel="external">Hadoop Application Architectures</a></li>
<li><a href="http://shop.oreilly.com/product/0636920033448.do" target="_blank" rel="external">Hadoop: The Definitive Guide</a></li>
<li><a href="http://www.slideshare.net/StampedeCon/choosing-an-hdfs-data-storage-format-avro-vs-parquet-and-more-stampedecon-2015" target="_blank" rel="external">Choosing an HDFS data storage format</a></li>
<li><a href="http://www.inquidia.com/news-and-info/hadoop-file-formats-its-not-just-csv-anymore" target="_blank" rel="external">Hadoop File Formats: It’s not just CSV anymore</a></li>
<li><a href="https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan" target="_blank" rel="external">100 open source Big Data architecture papers for data professionals</a></li>
<li><a href="https://github.com/ning/jvm-compressor-benchmark/wiki" target="_blank" rel="external">jvm compressor benchmark</a></li>
</ul>
</div></article></div></section><footer><div class="paginator"><a href="/2016/Spring-test-dbunit/" class="prev">PREV</a><a href="/2016/Hive-hdfs-file-slurper进行简单文本预处理/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2016 <a href="http://michaelhuang.github.io">huangzhf</a>, <a href="https://github.com/pinggod/hexo-theme-apollo">apollo</a>, <a href="http://hexo.io">hexo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-75326623-1",'auto');ga('send','pageview');</script></body></html>